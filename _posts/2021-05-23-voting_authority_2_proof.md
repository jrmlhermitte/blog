---
keywords: fastai
description: "This blog post attempts to prove the formula introduced in 'Extracting and Predicting Linked Social Media Accounts'"
title: Proof For Extracting and Predicting Linked Social Media Accounts
toc: true
branch: master
badges: true
comments: true
categories: [fastpages, jupyter]
hide: false
search_exclude: true
nb_path: _notebooks/2021-05-23-voting_authority_2_proof.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-05-23-voting_authority_2_proof.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Proof">Proof<a class="anchor-link" href="#Proof"> </a></h1><p>The previous post mentioned more or less quantitatively the formula:
$$Pr_{combined} = \frac{\prod_{i \in A_p} Pr_i \prod_{j \notin A_p} FNR_j}
{\prod_{i \in A_p} Pr_i\prod_{j \notin A_p} FNR_j + \prod_{i \in A_p} (1 - Pr_i) \prod_{j \notin A_p} (1 - FNR_j)}
$$</p>
<p>Now for the proof.</p>
<p>One way to think of this is in terms of sets.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Simple-Case:-Two-Authorities-vote-the-same-positive-result">Simple Case: Two Authorities vote the same positive result<a class="anchor-link" href="#Simple-Case:-Two-Authorities-vote-the-same-positive-result"> </a></h2><p>We will first consider the first case where we have only two authorities that vote for the same result. Let's say we have a universe of points $U$ of which some are true (green) and some false (red):
{% include image.html alt="CNN Example" max-width="200" file="/my_blog/images/copied_from_nb/images/2021_05_16_combined_authority_figa.png" %}</p>
<p>And within this universe, we have two authorities $A_1$ and $A_2$ which have a subset of points that they decide are true:</p>
<p>{% include image.html alt="CNN Example" max-width="200" file="/my_blog/images/copied_from_nb/images/2021_05_16_combined_authority_figb.png" %}</p>
<p>Here, the points that $A_1$ believes are true are denoted by the blue circle and the points that $A_2$ believes to be true are denoted by the purple circle. So points that $A_1$ and $A_2$ to both believe to be true would be represented by the overlap.</p>
<p>It should be noted that here, without loss of generality, we assume the points are arranged in such a way so that we may draw such a picture. Points that both $A_1$ and $A_2$ believe to be true that is also true are grouped close together as are the ones where $A_1$ and $A_2$ believe to be true but are in fact not true etc etc.</p>
<p>This image is not needed for the actual proof, but I think it helps give a better picture of what is going on.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Before we formalize this, let's count the combined precision of this set.</p>
<p>The combined precision is the number of true positives over the sum of true positives (TP) and false positives (FP):
$$
Pr = \frac{TP}{TP + FP}
$$
The TP are the overlap of the two authorities and what is true:
{% raw %}
$$TP = |T \cap A_1 \cap A_2|$$
{% endraw %}
The FP are the overlap of the two authorities with what is false:
{% raw %}
$$FP = |F \cap A_1 \cap A_2|$$
{% endraw %}</p>
<p>We can divide everything by all numbers in the universe of points $|U|$:
{% raw %}
$$Pr = \frac{\frac{TP}{|U|}}{\frac{TP}{|U|} + \frac{FP}{|U|}}$$
{% endraw %}</p>
<p>The values in the quotient and denominators become probabilities:
{% raw %}
$$\frac{TP}{|U|} = P(T \cap A_1 \cap A_2)$$
{% endraw %}
{% raw %}
$$\frac{FP}{|U|} = P(F \cap A_1 \cap A_2)$$
{% endraw %}
These probabilities can be rewritten in terms of conditional probabilities:
{% raw %}
$$\frac{TP}{|U|} = P(T | A_1 \cap A_2) P(A_1 \cap A_2)$$
{% endraw %}
and
{% raw %}
$$\frac{FP}{|U|} = P(F | A_1 \cap A_2) P(A_1 \cap A_2)$$
{% endraw %}</p>
<p>Now, this is nice, but it still leaves our final equation as this:
$$
\frac{TP}{TP + FP} = \frac{P(T | A_1 \cap A_2) P(A_1 \cap A_2)}{P(T | A_1 \cap A_2) P(A_1 \cap A_2) + P(F | A_1 \cap A_2) P(A_1 \cap A_2)}
$$
$$
\frac{TP}{TP + FP} = \frac{P(T | A_1 \cap A_2)}{P(T | A_1 \cap A_2) + P(F | A_1 \cap A_2) }$$</p>
<p>Since we have terms like $A_1 \cap A_2$, this equation still requires us to count how often $A_1$ and $A_2$ are right or wrong on the <strong>same training set</strong>. These kinds of measurements may not always be possible.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>However, if we can make one additional assumption that $A_1$'s decisions are completely unaffected by $A_2$ and vice versa (they are independent), the probabilities become:
{% raw %}
$$P(T | A_1 \cap A_2) = P(T | A_1) P(T | A_2)$$
{% endraw %}
{% raw %}
$$P(F | A_1 \cap A_2) = P(F | A_1) P(F | A_2)$$
{% endraw %}</p>
<p>The probability of an event being marked true given it is true is actually just the precision:
{% raw %}
$$P(T | A_1) = Pr_{A_1}$$
{% endraw %}
and that the inverse is just:
{% raw %}
$$P(F | A_1) = \frac{F \cap A_1}{U \cap A_1} = \frac{(U - T) \cap A_1}{U\cap A_1} = \frac{U\cap A_1}{U\cap A_1} - \frac{T \cap A_1}{U \cap A_1} = 1 - Pr_{A_1}$$
{% endraw %}</p>
<p>Combining this together, we get:
{% raw %}
$$Pr_{A_1, A_2} = \frac{Pr_{A_1} Pr_{A_2}}{Pr_{A_1} Pr_{A_2} + (1 - Pr_{A_1}) ( 1 - Pr_{A_2})}$$
{% endraw %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can imagine extending this proof to multiple authorities:
{% raw %}
$$A_1 \cap A_2 \rightarrow A_1 \cap A_2 \cap \cdots \cap A_i$$
{% endraw %}
Leading to:
{% raw %}
$$Pr_{A_1 \cdots A_i} = \frac{\prod_i Pr_{A_i}}{\prod_i Pr_{A_i} + \prod_i (1 - Pr_{A_i})}$$
{% endraw %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-case-where-some-authorities-vote-against-a-link">The case where some authorities vote against a link<a class="anchor-link" href="#The-case-where-some-authorities-vote-against-a-link"> </a></h2><p>When we have an authority that votes agains a link, the 
idea is similar, except the initial TP is:
{% raw %}
$$TP = (T \cup A_1 \cup \bar{A}_2)$$
{% endraw %}
where $\bar{A}_2 = U - A_2$.</p>
<p>The FP are also:
{% raw %}
$$FP = (F \cup A_1 \cup \bar{A}_2)$$
{% endraw %}</p>
<p>Finally, this leads to $P(T | \bar{A}_2) = FNR_{A_2}$.
I leave it up to the reader to fill in the gaps.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you have questions or comments, I would love to hear them!</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

